{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche du meillieur algo ML - Entranement test - pipeline¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des bibliothèques nécéssaires\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, classification_report\n",
    "import joblib\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour le chargement du dataSet à partir du fichier CSV\n",
    "def train_test_split_fonction(method, filename):\n",
    "    df= pd.read_csv(filename, sep=';')\n",
    "    if method == \"stem\":\n",
    "        X, y = df.StemmedComments, df.Sentiment\n",
    "    if method == \"lem\":\n",
    "        X, y = df.LemmatizedComments, df.Sentiment\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify = df.Sentiment)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_fonction_avec_gridsearch(classifier, vectorizer):\n",
    "    #  Paramètres GridSerach pour les vectorizers (Count and TFIDF)\n",
    "    parameters_vect = {\n",
    "        'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
    "        'vectorizer__max_features': [2000, 4000]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # paramètres GridSearch pour les Classifieurs \n",
    "    if classifier == \"log\":\n",
    "        parameters_clf = {\"classifier__C\":[0.01,0.1, 1, 10, 100],\n",
    "                          \"classifier__penalty\":['l1', 'l2']}  \n",
    "    elif classifier == \"rf\":\n",
    "        parameters_clf = {'classifier__n_estimators' : [10, 50, 100,200,500],\n",
    "                          'classifier__max_features': ['auto', 'log2', 'sqrt'], \n",
    "                          'classifier__criterion' :['gini', 'entropy']} \n",
    "    elif classifier == \"svm\" : \n",
    "        parameters_clf = {'classifier__kernel': ['linear', 'rbf'],\n",
    "                          'classifier__C':[0.01,0.1, 1, 10, 100], \n",
    "                          'classifier__gamma' : [0.001, 0.0001]}\n",
    "    else : \n",
    "        return \"Le classifieur spécifié n'est pas traité !\"\n",
    "    \n",
    "    \n",
    "    # définition du vectorizer \n",
    "    if vectorizer == 'tfidf' : \n",
    "        vect =  TfidfVectorizer()\n",
    "    elif vectorizer =='count' : \n",
    "        vect = CountVectorizer()\n",
    "    else :\n",
    "        return \"Le vectorizer spécifié n'est pas traité !\"\n",
    "    \n",
    "    \n",
    "    # définition du classifier \n",
    "    if classifier == \"log\": \n",
    "        clf = LogisticRegression()\n",
    "    elif classifier == \"rf\":\n",
    "        clf = RandomForestClassifier()\n",
    "    elif classifier == \"svm\" : \n",
    "        clf = SVC() \n",
    "    else : \n",
    "        return \"Le classifieur spécifié n'est pas traité !\"\n",
    "    \n",
    "    \n",
    "    # Création du Pipeline\n",
    "    # L'attribut \"memory\" permet d'activer la mise en cache des transformateurs du pipeline (CountVect/TFIDF) dans le dossier spécifié \n",
    "    # le tranformateur n'est pas calculé/entrainé à chaque fois, si les paramètres et les données d'entrée sont identiques (optimisation du temps de calcul) \n",
    "    cache_dir='./cachedir'\n",
    "    \n",
    "    sentiment_pipeline = Pipeline([\n",
    "        ('vectorizer', vect),\n",
    "        ('classifier', clf)\n",
    "    ], memory=cache_dir)\n",
    "    \n",
    "    \n",
    "    # Constuire l'objet GridSearchCV \n",
    "    # Définition de la liste compète des paramèters (hyperparamètres) de la GridSearchCV\n",
    "    gridParameters = dict()\n",
    "    gridParameters.update(parameters_vect)\n",
    "    gridParameters.update(parameters_clf)\n",
    "    \n",
    "    \n",
    "    # Création de l'objet GridSearch Associé \n",
    "    # P.S : n_jobs (nbre de processus à exécuter en parallèle ; n_jobs = -1 --> utiliser tous les processus du PC si celui-ci le permet)\n",
    "    # P.S: Score d'évaluation \"roc_auc\" pour prendre en compte le taux de faux postifs, en plus du taux des vrais positifs\n",
    "    #      --> Vue que le DataSet est déséquilibrée, cette métrique est mieux conseillée que l'accuracy pour la compraison des perf des algos. \n",
    "    # verbose = 1: pour afficher quelques message --> permet le suivi de l'avancement de l'algo\n",
    "    gs = GridSearchCV(sentiment_pipeline, param_grid=gridParameters,scoring='roc_auc', verbose=1, cv=5, n_jobs=-1)\n",
    "\n",
    "    # return l'objet GridSearch\n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fichier CSV contenant la DataSet à traiter \n",
    "dataFile = 'all_reviews_Fr_Prepared_binaryClass.csv'  # Commentaires Français\n",
    "#dataFile = 'all_reviews_En_Prepared_binaryClass.csv'  # Commentaires Anglais\n",
    "\n",
    "# Load & Split the Data \n",
    "# Données avec Stemmatisation \n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem = train_test_split_fonction(\"stem\", dataFile)\n",
    "\n",
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split_fonction(\"lem\", dataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va comparer la performance de 4 algorithmes ML différents connus pour l'analyse du Texte (logisticRegression; RandomForest; SVM; NaiveBayes)\n",
    "# Pour chaque algorithme , on fera appel à GridSearch pour optimiser ses hyperparamétres\n",
    "# Et cela dans les deux cas de traitement de texte : Stemming & Lemmatization et les deux vectorizers utilisés  \n",
    "\n",
    "# Création des différents pipelines nécéssaires \n",
    "# Avec Stemming\n",
    "gs_stem_count_rf = pipeline_fonction_avec_gridsearch('rf', 'count')\n",
    "gs_stem_tfidf_rf = pipeline_fonction_avec_gridsearch('rf', 'tfidf')\n",
    "gs_stem_count_log = pipeline_fonction_avec_gridsearch('log', 'count')\n",
    "gs_stem_tfidf_log = pipeline_fonction_avec_gridsearch('log', 'tfidf')\n",
    "gs_stem_count_svm = pipeline_fonction_avec_gridsearch('svm', 'count')\n",
    "gs_stem_tfidf_svm = pipeline_fonction_avec_gridsearch('svm', 'tfidf')\n",
    "# Avec lemmatization \n",
    "gs_lem_count_rf = pipeline_fonction_avec_gridsearch('rf', 'count')\n",
    "gs_lem_tfidf_rf = pipeline_fonction_avec_gridsearch('rf', 'tfidf')\n",
    "gs_lem_count_log = pipeline_fonction_avec_gridsearch('log', 'count')\n",
    "gs_lem_tfidf_log = pipeline_fonction_avec_gridsearch('log', 'tfidf')\n",
    "gs_lem_count_svm = pipeline_fonction_avec_gridsearch('svm', 'count')\n",
    "gs_lem_tfidf_svm = pipeline_fonction_avec_gridsearch('svm', 'tfidf')\n",
    "\n",
    "# Enregsitrement des pipelines dans une liste pour automatiser les itérations\n",
    "grids = [ gs_stem_count_rf,\n",
    "         gs_stem_tfidf_rf,\n",
    "         gs_stem_count_log, \n",
    "         gs_stem_tfidf_log,\n",
    "         gs_stem_count_svm,\n",
    "         gs_stem_tfidf_svm,\n",
    "         gs_stem_count_nb,\n",
    "         gs_stem_tfidf_nb,  \n",
    "         gs_lem_count_rf,\n",
    "         gs_lem_tfidf_rf,\n",
    "         gs_lem_count_log,\n",
    "         gs_lem_tfidf_log,\n",
    "         gs_lem_count_svm,\n",
    "         gs_lem_tfidf_svm\n",
    "        ]\n",
    "\n",
    "# Dictionnaire des pipelines et des types de classificateurs pour faciliter le référencement \n",
    "grid_dict = {0: 'Stemming - Random Forest/Countvectorizer',\n",
    "             1: 'Stemming - Random Forest/TFIDF Vectorizer',\n",
    "             2: 'Stemming - Logistic Regression/Countvectorizer',\n",
    "             3: 'Stemming - Logistic Regression/TFIDF Vectorizer', \n",
    "             4: 'Stemming - Support Vector Machine/Countvectorizer',\n",
    "             5: 'Stemming - Support Vector Machine/TFIDF Vectorizer',\n",
    "             6: 'Lemmatization - Random Forest/Countvectorizer',\n",
    "             7: 'Lemmatization - Random Forest/TFIDF Vectorizer',\n",
    "             8: 'Lemmatization - Logistic Regression/Countvectorizer',\n",
    "             9: 'Lemmatization - Logistic Regression/TFIDF Vectorizer', \n",
    "             10: 'Lemmatization - Support Vector Machine/Countvectorizer',\n",
    "             11: 'Lemmatization - Support Vector Machine/TFIDF Vectorizer'\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* Optimisation des Modèles ML et Réglages des Hyperparamètres ....**********\n",
      "\n",
      " Nouvelle instane GridSearch...\n",
      "Pipeline : Stemming - Random Forest/Countvectorizer\n",
      "------------------------------------------------\n",
      "Fitting 5 folds for each of 810 candidates, totalling 4050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 38.6min\n"
     ]
    }
   ],
   "source": [
    "# Fit the grid search objects\n",
    "print('********* Optimisation des Modèles ML et Réglages des Hyperparamètres ....**********')\n",
    "best_roc_auc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "\n",
    "# Boucle pour l'évaluation de tous les pipelines (enregistrement du meilleure Classifieur pour chaque pipeline )\n",
    "for idx, gs in enumerate(grids) : \n",
    "    \n",
    "    print(\"\\n Nouvelle instane GridSearch...\")\n",
    "    print('Pipeline : %s' % grid_dict[idx])\n",
    "    print('------------------------------------------------')\n",
    "    # Entrainement du Grid Search sur le data_train\n",
    "    t0 = time()\n",
    "    if (grid_dict[idx][0:3]=='Lem'): # fit on the LemmatizedComments\n",
    "        gs.fit(X_train_lem, y_train_lem)\n",
    "    else : \n",
    "        gs.fit(X_train_stem, y_train_stem)\n",
    "    print(\"Entrainement SearchGrid faut %0.4f\" % (time() - t0))\n",
    "    # Meilleur pipeline \n",
    "    best_pipe = gs.best_estimator_\n",
    "    # Meilleur paramètres et score \n",
    "    print('Meilleur Score CV d\\'entrainement (ROC AUC): %.4f' % gs.best_score_)   \n",
    "    print('Meilleur ensemble de paramètres: %s' % gs.best_params_)\n",
    "    # Prédire les donnes de Test avec le meilleur classifieur\n",
    "    if (grid_dict[idx][0:3]=='Lem'): # fit on the LemmatizedComments\n",
    "        y_pred = best_pipe.predict(X_test_lem)\n",
    "        roc_score = roc_auc_score(y_test_lem, y_pred)\n",
    "        print('Score AUC ROC sur les données de Test avec le meilleur pipeline : %0.4f ' % roc_score )\n",
    "        print('\\n')\n",
    "         # Afficher d'autres métriques significatives\n",
    "        print('Score \"Accuracy\" sur les données de Test avec le meilleur pipeline : %0.4f ' % accuracy_score(y_test_lem, y_pred) )\n",
    "        print('Matrice de Confusion :')\n",
    "        print(confusion_matrix(y_test_lem, y_pred))\n",
    "        print(\"Rapport de Classification sur les donénes de Test\")\n",
    "        print(classification_report(y_test_lem,y_pred))   \n",
    "    else :\n",
    "        y_pred = best_pipe.predict(X_test_stem)\n",
    "        roc_score = roc_auc_score(y_test_stem, y_pred)\n",
    "        print('Score AUC ROC  sur les données de Test avec le meilleur pipeline : %0.4f ' % roc_score )\n",
    "        print('\\n')\n",
    "        # Afficher d'autres métriques significatives\n",
    "        print('Score \"Accuracy\" sur les données de Test avec le meilleur pipeline : %0.4f ' % accuracy_score(y_test_stem, y_pred) )\n",
    "        print('Matrice de Confusion :')\n",
    "        print(confusion_matrix(y_test_stem, y_pred))\n",
    "        print(\"Rapport de Classification sur les donénes de Test\")\n",
    "        print(classification_report(y_test_stem,y_pred))\n",
    "    \n",
    "    #  Suivi du meilleur modèle (AUC ROC la plus élevée)\n",
    "    if roc_socre > best_roc_auc: \n",
    "        best_roc_auc = roc_socre \n",
    "        best_gs = gs \n",
    "        best_clf = idx \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choix du Meilleur Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Le Modèle avec les meilleurs performances est: %s' % grid_dict[best_clf])\n",
    "\n",
    "# Enregistrer le meilleur pipeline de GridSearch dans un fichier \n",
    "dump_file = 'best_gs_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_dict[best_clf], dump_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHOIX ET CHARGEMENT DU MODELE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir choisi notre modèle ainsi que les parameteres les plus optimals, il faut entrainer ce modèle sur toute la base puis sauvegarder le modèle sous forme de fichier .pkl grace au module pickle de Python.\n",
    "Pickle sert a serialiser, soit convertir une hiérarichie d'obtejts Python en un flux d'octet. Un tel fichier peut donc passer par le \"unpickling\" pour pouvoir permettre de manipuler le modèle en  python depuis un fihcir de flux d'octet.\n",
    "Cette étape est indispensable dans la mesure où le but de se projet et d'exposer un modèle ML. On aurait pu entrainer ce modèle dès que l'uilsatuer rentrerait un commentaire à analyser, mais cela serait bcp trop long et innaproprié. Pour cela il est donc indispensable d'enregistrer le modèle afin de deployer facilement un modèle de machine learning sur une interface (pour nous il s'agit d'un site web utilsant la librairie pytho FLASK)\n",
    "imaginons qu'il s'avere que log regression ait obtneu les meilleur score avec un tfidf sans lem ni stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/04/how-to-deploy-machine-learning-model-flask/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A mettre à jour cette partie une fois l'entrainement et la comparasion des modèle est terminé\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_model():\n",
    "    \n",
    "    X, y = df.StemmedComments, df.NoteLabel\n",
    "    #a modifier\n",
    "    vectorizer = TfidfVectorizer(stop_words=used_stop_words,lowercase=True,ngram_range=(1,3),max_features=2000)\n",
    "    #a modifier avec les patrametress\n",
    "    \n",
    "    classifier =  LogisticRegression(max_iter=1000)\n",
    "   \n",
    "    sentiment_pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier',classifier)\n",
    "    ])\n",
    "    \n",
    "    #fiting model with  training data\n",
    "    sentiment_pipeline.fit(X,y)\n",
    "    \n",
    "    #saving model to disk\n",
    "    pickle.dump(sentiment_pipeline, open('app/model_fr.pkl','wb'))\n",
    "    \n",
    "    #loading model to compare the result\n",
    "    model_fr = pickle.load(open('app/model_fr.pkl','rb'))\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test prediction grace au model chargé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-02 07:26:49,744 - spacy_lefff.lefff - INFO - New LefffLemmatizer instantiated.\n",
      "2020-10-02 07:26:49,745 - spacy_lefff.lefff - INFO - Token lefff_lemma already registered\n",
      "2020-10-02 07:26:49,747 - spacy_lefff.lefff - INFO - Reading lefff data...\n",
      "2020-10-02 07:26:50,321 - spacy_lefff.lefff - INFO - Successfully loaded lefff lemmatizer\n",
      "1\n",
      "0.48095825491136135\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "\n",
    "\n",
    "model_fr = pickle.load(open('app/model_fr.pkl','rb'))\n",
    "\n",
    "\n",
    "\n",
    "#si ni lemm ni stemm\n",
    "#text = [\"bravo\"]\n",
    "\n",
    "#si stemm\n",
    "text = \"honte\"\n",
    "FrenchStemmer = SnowballStemmer(\"french\")\n",
    "wt = word_tokenize(text)\n",
    "st = [FrenchStemmer.stem(token) for token in wt ]\n",
    "st = ' '.join(st)\n",
    "text = [st]\n",
    "\n",
    "\n",
    "#si lemm\n",
    "\"\"\"\n",
    "text = \"hontes\"\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "french_lemmatizer = LefffLemmatizer()\n",
    "nlp.add_pipe(french_lemmatizer, name='lefff')\n",
    "doc = nlp(text)   \n",
    "lt=''\n",
    "for d in doc:\n",
    "    #print(d.text, d.pos_, d._.lefff_lemma, d.tag_, d.lemma_)\n",
    "    lt = lt + \" \" + d.lemma_\n",
    "text = [lt]\n",
    "\"\"\"\n",
    "# predict the label using the pipeline\n",
    "print(str(model_fr.predict(text)[0]))\n",
    "\n",
    "\n",
    "proba = model_fr.predict_proba(text)[0]\n",
    "proba_neg = proba[0]\n",
    "proba_pos = proba[-1]\n",
    "print(proba_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
